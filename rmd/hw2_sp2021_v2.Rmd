---
title: "STAT 571/701 Modern Data Mining, HW 2"
author:
- Maddie Bannon 
- Chris Coelho 
- Charles Lachapelle 
date: 'Due: 11:59 PM,  Sunday, 02/28'
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '4'
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output
# check if you have ISLR package, if not, install it
if(!require('pacman')) {install.packages('pacman')}
pacman::p_load(ISLR, tidyverse, data.table, skimr, ggplot2, dplyr, leaps, car, GGally, reshape2) # add the packages needed
```


\pagebreak

# Overview {-}

Principle Component Analysis is widely used in data exploration, dimension reduction, data visualization. The aim is to transform original data into uncorrelated linear combinations of the original data while keeping the information contained in the data. High dimensional data tends to show clusters in lower dimensional view. 

Clustering Analysis is another form of EDA. Here we are hoping to group data points which are close to each other within the groups and far away between different groups. Clustering using PC's can be effective. Clustering analysis can be very subjective in the way we need to summarize the properties within each group. 

Both PCA and Clustering Analysis are so called unsupervised learning. There is no response variables involved in the process. 

For supervised learning, we try to find out how does a set of predictors relate to some response variable of the interest. Multiple regression is still by far, one of the most popular methods. We use linear models as a working model for its simplicity and interpretability. It is important that we use domain knowledge as much as we can to determine the form of the response as well as the function format of the factors.



## Objectives

- PCA
- SVD
- Clustering Analysis
- Linear Regression

## Review materials

- Study Module 2: PCA
- Study Module 3: Clustering Analysis
- Study lecture 4: Multiple regression

# Case study 1: Self-seteem 

Self-esteem generally describes a person's overall sense of self-worthiness and personal value. It can play significant role in one's motivation and success throughout the life. Factors that influence self-esteem can be inner thinking, health condition, age, life experiences etc. We will try to identify possible factors in our data that are related to the level of self-esteem. 

In the well-cited National Longitudinal Study of Youth (NLSY79), it follows about 13,000 individuals and numerous individual-year information has been gathered through surveys. The survey data is open to public [here](https://www.nlsinfo.org/investigator/). Among many variables we assembled a subset of variables including personal demographic variables in different years, household environment in 79, ASVAB test Scores in 81 and Self-Esteem scores in 81 and 87 respectively. 

The data is store in `NLSY79.csv`.

Here are the description of variables:

**Personal Demographic Variables**

* Gender: a factor with levels "female" and "male"
* Education05: years of education completed by 2005
* HeightFeet05, HeightInch05: height measurement. For example, a person of 5'10 will be recorded as HeightFeet05=5, HeightInch05=10.
* Weight05: weight in lbs.
* Income87, Income05: total annual income from wages and salary in 2005. 
* Job87, Job05: job type in 1987 and 2005, including Protective Service Occupations, Food Preparation and Serving Related Occupations, Cleaning and Building Service Occupations, Entertainment Attendants and Related Workers, Funeral Related Occupations, Personal Care and Service Workers, Sales and Related Workers, Office and Administrative Support Workers, Farming, Fishing and Forestry Occupations, Construction Trade and Extraction Workers, Installation, Maintenance and Repairs Workers, Production and Operating Workers, Food Preparation Occupations, Setters, Operators and Tenders,  Transportation and Material Moving Workers
 
 
**Household Environment**
 
* Imagazine: a variable taking on the value 1 if anyone in the respondent’s household regularly read magazines in 1979, otherwise 0
* Inewspaper: a variable taking on the value 1 if anyone in the respondent’s household regularly read newspapers in 1979, otherwise 0
* Ilibrary: a variable taking on the value 1 if anyone in the respondent’s household had a library card in 1979, otherwise 0
* MotherEd: mother’s years of education
* FatherEd: father’s years of education
* FamilyIncome78

**Variables Related to ASVAB test Scores in 1981**

Test | Description
--------- | ------------------------------------------------------
AFQT | percentile score on the AFQT intelligence test in 1981 
Coding | score on the Coding Speed test in 1981
Auto | score on the Automotive and Shop test in 1981
Mechanic | score on the Mechanic test in 1981
Elec | score on the Electronics Information test in 1981
Science | score on the General Science test in 1981
Math | score on the Math test in 1981
Arith | score on the Arithmetic Reasoning test in 1981
Word | score on the Word Knowledge Test in 1981
Parag | score on the Paragraph Comprehension test in 1981
Numer | score on the Numerical Operations test in 1981

**Self-Esteem test 81 and 87**

We have two sets of self-esteem test, one in 1981 and the other in 1987. Each set has same 10 questions. 
They are labeled as `Esteem81` and `Esteem87` respectively followed by the question number.
For example, `Esteem81_1` is Esteem question 1 in 81.

The following 10 questions are answered as 1: strongly agree, 2: agree, 3: disagree, 4: strongly disagree

* Esteem 1: “I am a person of worth”
* Esteem 2: “I have a number of good qualities”
* Esteem 3: “I am inclined to feel like a failure”
* Esteem 4: “I do things as well as others”
* Esteem 5: “I do not have much to be proud of”
* Esteem 6: “I take a positive attitude towards myself and others”
* Esteem 7: “I am satisfied with myself”
* Esteem 8: “I wish I could have more respect for myself”
* Esteem 9: “I feel useless at times”
* Esteem 10: “I think I am no good at all”

## Data preparation

Load the data. Do a quick EDA to get familiar with the data set. Pay attention to the unit of each variable. Are there any missing values?


```{r}
getwd()
setwd("C:/Users/38MMB/Documents/hw-2")
SE_data <- read.csv("data/NLSY79.csv")
```


```{r}
#identifying missing variables 
skim(SE_data)
str(SE_data)

#identifying missing values
SE_data %>%
  group_by(Job05) %>%
  tally()

#address negative values in income 

SE_data$Income05[SE_data$Income05 < 0] <- mean(SE_data$Income05)
SE_data$Income87[SE_data$Income87 < 0] <- mean(SE_data$Income87)

SE_data["ln_Income87"] <- log((SE_data$Income87+1))
SE_data["ln_Income05"] <- log((SE_data$Income05+1))

```

Observations 
1. Negative values for income suggests missing data
3. Given skewed distribution of income, transformed into log scale 
2. Empty job descriptions

## Self esteem evaluation

Let concentrate on Esteem scores evaluated in 87. 

1. Reverse Esteem 1, 2, 4, 6, and 7 so that a higher score corresponds to higher self-esteem. (Hint: if we store the esteem data in `data.esteem`, then `data.esteem[,  c(1, 2, 4, 6, 7)]  <- 5 - data.esteem[,  c(1, 2, 4, 6, 7)]` to reverse the score.)

```{r}
# to understand column index
data.frame(colnames(SE_data)) 
head(SE_data, 5)

#reversing scores 
data.esteem.87 <- SE_data[c(37:46)]
data.esteem.87[,  c(1, 2, 4, 6, 7)]  <- 5 - data.esteem.87[,  c(1, 2, 4, 6, 7)]

#confirming results
head(data.esteem.87, 5)
```

2. Write a brief summary with necessary plots about the 10 esteem measurements.
```{r}

#reshape data to long form vs. wide

data.esteem.87.short <- melt (data.esteem.87, measure.vars =c("Esteem87_1", "Esteem87_2", "Esteem87_3", "Esteem87_4", "Esteem87_5", "Esteem87_6", "Esteem87_7", "Esteem87_8", "Esteem87_9", "Esteem87_10"), variable.name = "Esteem", value.name = "Score") 

head(data.esteem.87.short,5)

#create boxplot
SE_87_boxplot <- ggplot(data.esteem.87.short, aes(x=Esteem, y=Score)) 
SE_87_boxplot + geom_boxplot() 

#Aggregate analysis 

SE_data_87_mean <- colMeans(data.esteem.87)
SE_data_87_mean <-SE_data_87_mean[order(SE_data_87_mean)]
SE_data_87_mean

#Disaggregated analysis 

#create histogram Esteem 1 
SE_87_histplot1 <- ggplot(data.esteem.87, aes(data.esteem.87$Esteem87_1)) 
SE_87_histplot1 + geom_histogram() 

#create histogram Esteem 2 
SE_87_histplot2 <- ggplot(data.esteem.87, aes(data.esteem.87$Esteem87_2)) 
SE_87_histplot2 + geom_histogram() 

#create histogram Esteem 9
SE_87_histplot9 <- ggplot(data.esteem.87, aes(data.esteem.87$Esteem87_9)) 
SE_87_histplot9 + geom_histogram() 

#create scatter plot Esteem 1 vs. Esteem 2
SE_87_scatplot12 <- ggplot(data.esteem.87, aes(x=Esteem87_1, y=Esteem87_2)) 
SE_87_scatplot12 + geom_point() 


#create scatter plot Esteem 1 vs. Esteem 3
SE_87_scatplot13 <- ggplot(data.esteem.87, aes(x=Esteem87_1, y=Esteem87_3)) 
SE_87_scatplot13 + geom_point() 


#create scatter plot Esteem 1 vs. Esteem 4
SE_87_scatplot14 <- ggplot(data.esteem.87, aes(x=Esteem87_1, y=Esteem87_4)) 
SE_87_scatplot14 + geom_point() 

#create scatter plot Esteem 1 vs. Esteem 10
SE_87_scatplot110 <- ggplot(data.esteem.87, aes(x=Esteem87_1, y=Esteem87_10)) 
SE_87_scatplot110 + geom_point() 


```
Key takeaways:
(1) Esteem Variables 1-3 have higher average scores relative to the other Esteem Variables, in particular Esteem 7-10. 
(2) This is also suggested in the historgrams, which show that Esteem Values 1-3 are heavily skewed left, whereas 7-10 include greater concerntration in the 1 & 2 score buckets. 
(3) Given the discrete nature of the score values, it's difficult to discern significant insights from the scatter plots of individual Esteem Values. 


3. Do esteem scores all positively correlated? Report the pairwise correlation table and write a brief summary.
```{r}
data.esteem.87.corr <- cor(data.esteem.87)
data.esteem.87.corr
```

The correlation tables suggests that all of the esteem variables are positively correlated. However, some are more directionally correlated than others. As suggested in the EDA insights above, Esteem variables 1-5 are more positively correlated with eachother than they are with variables 6-10. The inverse relationship is true for variables 6-10 relative to eachother and relative to 1-5.  


4. PCA on 10 esteem measurements. (centered but no scaling)

    a) Report the PC1 and PC2 loadings. Are they unit vectors? Are they uncorrelated? 
    
See in below output for PC1 and PC2 loadings. No, they are not unit vectors because they are not scaled. No, the loadings for PC1 and PC2 are not uncorrelated. 

```{r}
#Perform PCA
PCA.SE.87 <- prcomp(data.esteem.87)  # by default, center=True but scale=FALSE!!!

#Analyze loadings
PCA.SE.87.loading <- PCA.SE.87$rotation
PCA.SE.87.loading[,1:2]

#Calculating correlation
cor(PCA.SE.87.loading[,1:2])

```
  
    b) Are there good interpretations for PC1 and PC2? (If loadings are all negative, take the positive loadings for the ease of interpretation)

See above. The loadings for PC1 are relatively similar across each of the Esteem variables, with slightly more weight going towards esteem variables 8-10. A higher PC1 score will result in a higher overall score. 
    
The loadings for PC2 suggest that an overall score is significanlty more weighted towards Esteem Variables 8-10. 
  
    c) How is the PC1 score obtained for each subject? Write down the formula.
    
The PC1 score is calculated as follows: PC1= `PCA.SE.87.loading[1,1]` Esteem87_1_centered + ``PCA.SE.87.loading[2,1]` Esteem87_2_centered + 
... + `PCA.SE.87.loading[10,1]` Esteem87_10_centered
    
    d) Are PC1 scores and PC2 scores in the data uncorrelated? 

Yes, the PC1 and PC2 scores are uncorrelated. 
    
```{r}
PCA.SE.87.x <- PCA.SE.87$x[,1:2]
round(cor(PCA.SE.87.x),10)
```
    
    
    e) Plot PVE (Proportion of Variance Explained) and summarize the plot. 
    
The PVE plot shows that PC1 explains the largest proportion of variance at ~47%. Between PC1 and PC2, the proporiton of variance explained drops significnalty to ~13%. Additionally, with each subsquent PCA variable, the proportion of variance explained decreases.    

```{r}

#Summary of importance
summary(PCA.SE.87)$importance


#Plot of SVE
plot(summary(PCA.SE.87)$importance[2,],  
     ylab="PVE",
     xlab="Number of PC's",
     pch = 16, 
     main="Scree Plot of PVE for SE 87")

```
  
    f) Also plot CPVE (Cumulative Proportion of Variance Explained). What proportion of the variance in the data is explained by the first two principal components?
    
    ~60% of the variation is explained by the first two prinicipal compnents. 
    
```{r}
  plot(summary(PCA.SE.87)$importance[3,],  
     ylab="PVE",
     xlab="Number of PC's",
     pch = 16, 
     main="Scree Plot of Cum PVE for SE 87")
```
    
    g) PC’s provide us with a low dimensional view of the self-esteem scores. Use a biplot with the first two PC's to display the data.  Give an interpretation of PC1 and PC2 from the plot. (try `ggbiplot` if you could, much prettier!)
    
```{r}
lim <- c(-.1, .1) 
biplot(PCA.SE.87,
       expand = 2,
       xlim=lim,
       ylim=lim,
       main="Biplot of the PC's")
abline(v=0, h=0)
```
    

5. Apply k-means to cluster subjects on the original esteem scores

    a) Find a reasonable number of clusters using within sum of squared with elbow rules.
  
Using the elbow method, the optimal number of clusters is 3 (i.e., k=3). 
    
```{r}

#Analyzing impact of different k values (not using built-in functionality )
set.seed(0)

wss <- function(data.esteem.87, k) {
  kmeans(data.esteem.87, k, k, nstart = 10)$tot.withinss
}

k.values <- 2:15

wss_values <- sapply(k.values, 
                     function(k) kmeans(data.esteem.87[,-1], centers = k)$tot.withinss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")

#clearer to select 3
fviz_nbclust(data.esteem.87, kmeans, method = "wss")

#Applying k-means to cluster subjects 
SE.data.87.kmeans <- data.esteem.87 %>% 
    kmeans(centers = 3 )  # centers: number of cluster
str(SE.data.87.kmeans) 

```
    
    b) Can you summarize common features within each cluster?
    
```{r}

#Applying k-means to cluster subjects 
SE.data.87.kmeans <- data.esteem.87 %>% 
    kmeans(centers = 3 )  # centers: number of cluster
str(SE.data.87.kmeans) 

#Calculating cluster averages
SE_group_mean <- data.esteem.87 %>% 
    mutate(mean = rowMeans(data.esteem.87)) %>%
    mutate(group = SE.data.87.kmeans$cluster) %>%
    select(group, mean)

SE_group_mean_table <- table(SE_group_mean)    
print(SE_group_mean_table)

data.esteem.87 %>% 
    mutate(mean = rowMeans(data.esteem.87)) %>%
    mutate(group = SE.data.87.kmeans$cluster) %>%
    select(group, mean) %>%
    group_by(group) %>%
    summarise(group_mean = mean(mean)) 


#Calculating cluster averages
data.esteem.87 %>% 
    mutate(mean = rowMeans(data.esteem.87)) %>%
    mutate(group = SE.data.87.kmeans$cluster) %>%
    select(group, mean) %>%
    group_by(group) %>%
    summarise(group_SD = sd(mean))


```

Key observations:
(1) Cluster 1 has the largest mean, followed by Cluster 2, and then Cluster 3, suggesting that cluster 1 has on average higher overall esteem scores across each of the esteem variables. 
(2) Cluster 1 is heavily skewed left, and thus biased towards higher averages, whereas Cluster 2 and Cluster 3 are normally distributed acrosse a range of values
(3) While Cluster 2 generally has high scores on average, occasionally low scores occur in Esteem variables 5-10. Whereas, Cluster 3 on average incldues a mix of low and high scores 

    
    c) Can you visualize the clusters with somewhat clear boundaries? You may try different pairs of variables and different PC pairs of the esteem scores.
    
```{r}

#Exploratory visualizations (not using PCA)
data.frame(
          Esteem_1 = data.esteem.87[,1],
          Esteem_2 = data.esteem.87[,2],
          group = as.factor(SE.data.87.kmeans$cluster)) %>%
  ggplot(aes(x = Esteem_1, y = Esteem_2, col = group)) +
  geom_point() +
  ggtitle("Clustering over two variables")


data.frame(
          Esteem_1 = data.esteem.87[,1],
          Esteem_10 = data.esteem.87[,10],
          group = as.factor(SE.data.87.kmeans$cluster)) %>%
  ggplot(aes(x = Esteem_1, y = Esteem_10, col = group)) +
  geom_point() +
  ggtitle("Clustering over two variables")

data.frame(
          Esteem_3 = data.esteem.87[,3],
          Esteem_6 = data.esteem.87[,6],
          group = as.factor(SE.data.87.kmeans$cluster)) %>%
  ggplot(aes(x = Esteem_3, y = Esteem_6, col = group)) +
  geom_point() +
  ggtitle("Clustering over two variables")

```

It is difficult to visualize the clusters by plotting each of the esteem variables due to the discrete nature of the scores. However, we can generally see that Group 1 has higher scores across the variables relative to 2 and 3. Further cluster 2 scores are typically greater than group 3. 

```{r}

#Exploratory visualizations with PCA 
data.frame(
          PCA1 =  PCA.SE.87.x[,1],
          PCA2 =  PCA.SE.87.x[,2],
          group = as.factor(SE.data.87.kmeans$cluster)) %>%
  ggplot(aes(x = PCA1, y = PCA2, col = group)) +
  geom_point() +
  ggtitle("Clustering over two variables")
```

However, using results of the PCA analysis we are able to better visualize the relationships between each of the Esteem Variables. Our Analysis still stands however, showing higher overall scores for group 1, relative to the other clusters. Further, we can see the impact of higher scores for Esteem variables 1-4 in the higher overall PCA1 scores for group 2 relative to group 4. 

6. We now try to find out what factors are related to self-esteem? PC1 of all the Esteem scores is a good variable to summarize one's esteem scores. We take PC1 as our response variable. 

    a) Prepare possible factors:

      - Personal information: gender, education, log(income), job type, Body mass index as a measure of health (The BMI is defined as the body mass divided by the square of the body height, and is universally expressed in units of kg/m²)
          
      - Household environment: Imagazine, Inewspaper, Ilibrary, MotherEd, FatherEd, FamilyIncome78. Do set indicators `Imagazine` and `Ilibrary` as factors
    
      - Use PC1 of SVABS as level of intelligence
      
```{r}

colnames(SE_data)
#creating BMI variable 
SE_data["BMI"] <- (703 * SE_data$Weight05) / ((SE_data$HeightFeet05*12) + SE_data$HeightInch05)^2

#creating ln income
SE_data$Income05[SE_data$Income05 < 0] <- mean(SE_data$Income05)
SE_data$Income87[SE_data$Income87 < 0] <- mean(SE_data$Income87)

SE_data["ln_Income87"] <- log((SE_data$Income87+1))
SE_data["ln_Income05"] <- log((SE_data$Income05+1))

#Creating PC1
PC1 <- PCA.SE.87.x[,1]



#Creating dataframe for regression
SE_data_lm <- data.frame(
  gender = SE_data$Gender,
  PC1 = PC1,
  ln_Income87 = SE_data$ln_Income87,
  Imag = SE_data$Imagazine, 
  Ilib = SE_data$Ilibrary, 
  Inews = SE_data$Inewspaper, 
  MomEd = SE_data$MotherEd, 
  DadEd = SE_data$FatherEd,
  FamIncome = SE_data$FamilyIncome78
)


#Converting Imag and Ilib to factors 
SE_data_lm$Ilib <- as.factor(SE_data_lm$Ilib)
SE_data_lm$Imag <- as.factor(SE_data_lm$Imag)
SE_data_lm$Inews <- as.factor(SE_data_lm$Inews)



```

        
    b)   Run a few regression models between PC1 of all the esteem scores and factors listed in a). Find a final best model with your own criterion. 
    
```{r}

#calculating correlation plot 
cor(SE_data_lm[sapply(SE_data_lm, is.numeric)])
#Mom/DadEd highly correlated 

#pairwise analysis
SE_data_lm %>%   
  select_if(is.numeric) %>%
  select(ln_Income87, PC1, MomEd, DadEd) %>%
  ggpairs()


```

```{r}
#model evaluation: Comparing impact of highly correlated variables 
model_Dad <- lm(PC1 ~ DadEd, data=SE_data_lm)
summary(model_Dad)

model_Mom <- lm(PC1 ~ MomEd, data=SE_data_lm)
summary(model_Mom)

#model 1: all variables
model_1 <- lm(PC1 ~ ., data=SE_data_lm)
summary(model_1)

#using regsubsets
fit.SE <- regsubsets(PC1 ~., SE_data_lm , nvmax=25, method="exhaustive")
summary(fit.SE)

f.e <- summary(fit.SE)

#fit CP values
plot(f.e$cp, xlab="Number of predictors",
ylab="Cp", col="red", pch=16)


#final model
final <- lm(PC1 ~ . -DadEd, data=SE_data_lm)
summary(final)

```


      - How did you land this model? Run a model diagnosis to see if the linear model assumptions are reasonably met. 
      
We selected variables based on the logic of how they might influence self-esteem -- e.g., we did not include BMI as it is only available for 2005, which would not influece the self esteem scores in previous years because weight may change. Our final model was selected using cp analysis, which suggested that using all of the variables would minimize prediction error the most. However, we decided to exclude the DadEd variable as it was highly correlated with MomEd, presenting the issue of multicollinearity. 
        
      - Write a summary of your findings. In particular, explain what and how the variables in the model affect one's self-esteem. 
        
Using our final model, we conclude that Family related variables in general have the greatest impact on your self-esteem. Specifically, Whether someone in your familyt reguarly red the news, as well as the education level of your mother. Additionally, gender also impacts your self-esteem, with males having a slightly higher self-esteem relative to females. 


# Case study 2: Breast cancer sub-type


[The Cancer Genome Atlas (TCGA)](https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga), a landmark cancer genomics program by National Cancer Institute (NCI), molecularly characterized over 20,000 primary cancer and matched normal samples spanning 33 cancer types. The genome data is open to public from the [Genomic Data Commons Data Portal (GDC)](https://portal.gdc.cancer.gov/).
 
In this study, we focus on 4 sub-types of breast cancer (BRCA): basal-like (basal), Luminal A-like (lumA), Luminal B-like (lumB), HER2-enriched. The sub-type is based on PAM50, a clinical-grade luminal-basal classifier. 

* Luminal A cancers are low-grade, tend to grow slowly and have the best prognosis.
* Luminal B cancers generally grow slightly faster than luminal A cancers and their prognosis is slightly worse.
* HER2-enriched cancers tend to grow faster than luminal cancers and can have a worse prognosis, but they are often successfully treated with targeted therapies aimed at the HER2 protein. 
* Basal-like breast cancers or triple negative breast cancers do not have the three receptors that the other sub-types have so have fewer treatment options.

We will try to use mRNA expression data alone without the labels to classify 4 sub-types. Classification without labels or prediction without outcomes is called unsupervised learning. We will use K-means and spectrum clustering to cluster the mRNA data and see whether the sub-type can be separated through mRNA data.

We first read the data using `data.table::fread()` which is a faster way to read in big data than `read.csv()`. 

```{r}
brca <- fread("data/brca_subtype.csv")

# get the sub-type information
brca_subtype <- brca$BRCA_subtype_PAM50
brca <- brca[,-1]
```

1. Summary and transformation

    a) How many patients we have in each sub-type? 

```{r}
# sum all patients for each of the sub-type
sum(brca_subtype == "LumA")
sum(brca_subtype == "LumB")
sum(brca_subtype == "Her2")
sum(brca_subtype == "Basal")
```

There are 628 patients in LumA, 233 in LumB, 91 in Her2, and 208 in Basal
 
    b) Randomly pick 5 genes and plot the histogram by each sub-type.
    
```{r}
num_gene <- ncol(brca)

# Randomly select 5 genes
set.seed(5)
sample_idx <- sample(num_gene, 5)

# Plot the count number histogram for each gene
brca %>%
 select(all_of(sample_idx)) %>% # select column by index
 pivot_longer(cols = everything()) %>% # for facet(0)
 ggplot(aes(x = value, y = ..density..)) +
 geom_histogram(aes(fill = name)) +
 facet_wrap(~name, scales = "free") +
 theme_bw() +
 theme(legend.position = "none")


```    

    c) Remove gene with zero count and no variability. Then apply logarithmic transform.

```{r}
# remove genes with 0 counts
sel_cols <- which(colSums(abs(brca)) != 0)
brca_sub <- brca[, sel_cols, with=F]
dim(brca_sub)
```

```{r}
# Apply log(x + 1e − 10) to each cell to avoid values which are very close to 0
brca_sub <- log2(as.matrix(brca_sub+1e-10))

```


2. Apply kmeans on the transformed dataset with 4 centers and output the discrepancy table between the real sub-type `brca_subtype` and the cluster labels.

```{r}
# Run kmeans (System.time() to get how long it runs for)
system.time({brca_sub_kmeans <- kmeans(x = brca_sub, 4)}) 

# discrepancy table
table(brca_subtype, brca_sub_kmeans$cluster)


```


3. Spectrum clustering: to scale or not to scale?

    a) Apply PCA on the centered and scaled dataset. How many PCs should we use and why? You are encouraged to use `irlba::irlba()`.
    
```{r}
# Calculate the PCA for centered and scaled data
pca_ret <- prcomp(brca_sub, center = T, scale. = T)

pca_ret$rotation <- pca_ret$rotation[, 1:20]
pca_ret$x <- pca_ret$x[, 1:20]

# Plot scree plot of PVE
pve <- summary(pca_ret)$importance[2, 1:10]
plot(pve, type="b", pch = 19, frame = FALSE)


```

Based on the above plot, 4PCs should be used as there is a sharp drop in PVE between 3 and 4 PCs
    
    b) Plot PC1 vs PC2 of the centered and scaled data and PC1 vs PC2 of the centered but unscaled data side by side. Should we scale or not scale for clustering propose? Why? (Hint: to put plots side by side, use `gridExtra::grid.arrange()` or `ggpubr::ggrrange()` or `egg::ggrrange()` for ggplots; use `fig.show="hold"` as chunk option for base plots)
    
```{r}
# Find the PCA for centered and unscaled data
pca_ret2 <- prcomp(brca_sub, center = T, scale. = F)

pca_ret2$rotation <- pca_ret2$rotation[, 1:20]
pca_ret2$x <- pca_ret2$x[, 1:20]
```


```{r}
kmean_ret <- kmeans(x = pca_ret$x[, 1:4], 4)

# Centered and scaled chart

p <- data.table(x = pca_ret$x[,1],
 y = pca_ret$x[,2],
 col = as.factor(brca_subtype),
 cl = as.factor(kmean_ret$cluster)) %>%


ggplot() +
 geom_point(aes(x = x, y = y, col = col, shape = cl)) +
 
 theme_bw() +
 labs(color = "Cancer type", shape = "Cluster") +
 labs(x="PC1", y="PC2",
 title="Centered and Scaled")
 p

# Centered and unscaled chart
kmean_ret2 <- kmeans(x = pca_ret2$x[, 1:4], 4)

p2 <- data.table(x = pca_ret2$x[,1],
  y = pca_ret2$x[,2],
 col2 = as.factor(brca_subtype),
 cl2 = as.factor(kmean_ret2$cluster)) %>%

ggplot() +
 geom_point(aes(x = x, y = y, col = col2, shape = cl2)) +
 
theme_bw() +
labs(color = "Cancer type", shape = "Cluster") +
labs(x="PC1", y="PC2", title="Centered and Unscaled")
p2
```

The scales of the genes are very different as we can see in the histograms. However, runing the cluster analysis with both methods shows a better fit when scaling is not applied.    
    

4. Spectrum clustering: center but do not scale the data

    a) Use the first 4 PCs of the centered and unscaled data and apply kmeans. Find a reasonable number of clusters using within sum of squared with the elbow rule.

```{r}
# Run kmeans on centered and unscaled data
kmean_ret2 <- kmeans(x = pca_ret2$x[, 1:4], 4)



set.seed(0)

# function to compute total within-cluster sum of square
wss <- function(df, k) {
kmeans(df, k, nstart = 10)$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 2:15

# extract wss for 2-15 clusters using sapply
wss_values <- sapply(k.values,
function(k) kmeans(brca_sub[,-1], centers = k)$tot.withinss)

plot(k.values, wss_values,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")


```

The results of the elbow analysis suggest that 4 is the optimal number of clusters as it appears to be the bend in the elbow.
    
    b) Choose an optimal cluster number and apply kmeans. Compare the real sub-type and the clustering label as follows: Plot scatter plot of PC1 vs PC2. Use point color to indicate the true cancer type and point shape to indicate the clustering label. Plot the kmeans centroids with black dots. Summarize how good is clustering results compared to the real sub-type.

```{r}
# Centered and unscaled chart
kmean_ret2 <- kmeans(x = pca_ret2$x[, 1:4], 4)

p2 <- data.table(x = pca_ret2$x[,1],
y = pca_ret2$x[,2],
col2 = as.factor(brca_subtype),
cl2 = as.factor(kmean_ret2$cluster)) %>%

ggplot() +
geom_point(aes(x = x, y = y, col = col2, shape = cl2)) +
theme_bw() +
labs(color = "Cancer type", shape = "Cluster") +
labs(x="PC1",
y="PC2",
title="Centered and Unscaled")
p2
 
```

The clustering seems to be good for the Basal type but doesn't seem to fit well with the other cancer types as there is overlap between the clusters and the real types.


    c) Compare the clustering result from applying kmeans to the original data and the clustering result from applying kmeans to 4 PCs. Does PCA help in kmeans clustering? What might be the reasons if PCA helps?
    
```{r}

# discrepancy tables
table(brca_subtype, brca_sub_kmeans$cluster)

table(brca_subtype, kmean_ret2$cluster)



``` 
In this case there is no significant impact of the PCA on the data set as there is still noise within the subtypes. PCA can help in clustering as it reduces the noise and redundancy in the data by reducing the original data into a smaller number of principal components. It is also helpful in identifying correlated variables.    
    
    
    d) Now we have an x patient with breast cancer but with unknown sub-type. We have this patient's mRNA sequencing data. Project this x patient to the space of PC1 and PC2. (Hint: remember we remove some gene with no counts or no variablity, take log and centered) Plot this patient in the plot in iv) with a black dot. Calculate the Euclidean distance between this patient and each of centroid of the cluster. Can you tell which sub-type this patient might have? 
    
```{r}
# Read patient data
x_patient <- fread("C:/Users/Admin/Documents/Wharton/Courses/Spring 2021/STAT 701/HW2/hw-2/data/brca_x_patient.csv")

# remove genes with 0 counts
sel_cols <- which(colSums(abs(x_patient)) != 0)
patient_sub <- x_patient[, sel_cols, with=F]
dim(patient_sub)

# Apply log(x + 1e − 10) to each cell to avoid values which are very close to 0
patient_sub <- log2(as.matrix(patient_sub+1e-10))

# Find the PCA for centered and unscaled data
pca_ret_patient <- prcomp(patient_sub, center = T, scale. = F)

pca_ret_patient$rotation <- pca_ret_patient$rotation[, 1:1]
pca_ret_patient$x <- pca_ret_patient$x[, 1:1]


x_patient_pca_score_1 <- patient_sub * pca_ret_patient$rotation [,1]
x_patient_pca_score_2<- patient_sub * pca_ret_patient$rotation [,2]

```


# Case study 3: Auto data set

This question utilizes the `Auto` dataset from ISLR. The original dataset contains 408 observations about cars. It is similar to the CARS dataset that we use in our lectures. To get the data, first install the package ISLR. The `Auto` dataset should be loaded automatically. We'll use this dataset to practice the methods learn so far. Original data source is here: <https://archive.ics.uci.edu/ml/datasets/auto+mpg>

Get familiar with this dataset first. Tip: you can use the command `?ISLR::Auto` to view a description of the dataset.

```{r load auto data, message=FALSE, echo=FALSE, message=FALSE}
pacman::p_load(ISLR)
cs3.auto<- Auto
```

## EDA

Explore the data, with particular focus on pairwise plots and summary statistics. Briefly summarize your findings and any peculiarities in the data.

```{r Auto EDA, message=FALSE, warning=FALSE}
pacman::p_load(gridExtra)
#Check Variable Names
names(cs3.auto)
#view variables 
skim(cs3.auto)
cs3.auto%>% 
  group_by(year) %>% 
  tally()
#summarize variables 
cs3.auto %>% summary()

#compare pairwise plots 
p.mpg_cyl<-cs3.auto %>% ggplot(aes(x=cylinders, y=mpg, color=origin)) +
  geom_point() +
  geom_smooth(method="lm", formula=y~x, se=F, color="red") + 
  #facet_wrap(~year)+
  theme_bw()+
  theme(legend.position="none")+ 
  labs(title="mpg on cylinders")
p.mpg_dis<-cs3.auto %>% ggplot(aes(x=displacement, y=mpg, color=origin)) +
  geom_point() +
  geom_smooth(method="lm", formula=y~x, se=F, color="red") + 
  #facet_wrap(~year)+
  theme_bw()+
  theme(legend.position="none")+ 
  labs(title="mpg on displacement")
p.mpg.horse<-cs3.auto %>% ggplot(aes(x=horsepower, y=mpg, color=origin)) +
  geom_point() +
  geom_smooth(method="lm", formula=y~x, se=F, color="red") + 
  #facet_wrap(~year)+
  theme_bw()+
  theme(legend.position="none")+ 
  labs(title="mpg on horsepower")
p.mpg_weight<-cs3.auto %>% ggplot(aes(x=weight, y=mpg, color=origin)) +
  geom_point() +
  geom_smooth(method="lm", formula=y~x, se=F, color="red") + 
  #facet_wrap(~year)+
  theme_bw()+
  theme(legend.position="none")+ 
  labs(title="mpg on weight")
p.mpg_acc<-cs3.auto %>% ggplot(aes(x=acceleration, y=mpg, color=origin)) +
  geom_point() +
  geom_smooth(method="lm", formula=y~x, se=F, color="red") + 
  #facet_wrap(~year)+
  theme_bw()+
  theme(legend.position="none")+ 
  labs(title="mpg on acceleration")
p.mpg_yr<-cs3.auto %>% ggplot(aes(x=year, y=mpg, color=origin)) +
  geom_point() +
  geom_smooth(method="lm", formula=y~x, se=F, color="red") + 
  #facet_wrap(~year)+
  theme_bw()+
  theme(legend.position="none")+ 
  labs(title="mpg on year")
p.mpg_origin<-cs3.auto %>% ggplot(aes(x=origin, y=mpg, color=origin)) +
  geom_point() +
  geom_smooth(method="lm", formula=y~x, se=F, color="red") + 
  #facet_wrap(~year)+
  theme_bw()+
  theme(legend.position="none")+ 
  labs(title="mpg on origin")
gridExtra::grid.arrange(p.mpg_cyl, p.mpg_dis, p.mpg.horse, p.mpg_weight, p.mpg_acc, p.mpg_yr, p.mpg_origin, ncol=4)

# using pairs
cs3.pairwise<-cs3.auto %>%
  select(-year, -origin, -name) %>%
  pairs()

# logging MPG
par(mfrow=c(2,2))
plot(cs3.auto$displacement, cs3.auto$mpg, pch=16)
plot(1/cs3.auto$displacement, cs3.auto$mpg, pch=16) # looks better!
  cs3.auto$disp.2<-1/cs3.auto$displacement
plot(cs3.auto$horsepower, cs3.auto$mpg, pch=16)
plot(1/cs3.auto$horsepower, cs3.auto$mpg, pch=16) # looks better! 
   cs3.auto$horse.2<-1/cs3.auto$displacement

```

### EDA Answer

-   We saw no missing data through either the ::skim:: or ::tally:: functions\

-   Using the summary function, we observed:

-   `acceleration`, `year` were somewhat normal.

-   `origin`, and `name` appeared to be categorical and factor variables respectively\

-   In doing a pairwise comparison, we noticed:

    -   `mpg` interacted with `displacement` and `horsepower` in a way that may be non-linear. To correct this we compared them to 1/the data, and these variables looked better so we stored them.

## What effect does `time` have on `MPG`?

a)  Start with a simple regression of `mpg` vs. `year` and report R's `summary` output. Is `year` a significant variable at the .05 level? State what effect `year` has on `mpg`, if any, according to this model.

```{r time on MPG simple, message=FALSE, warning=FALSE}
#build model
cs3.lm.a<-lm(mpg~year, data=cs3.auto)
# print model
print(summary(cs3.lm.a))
#check confidence intervals 
confint(cs3.lm.a, level=.95)  
#plot model diagnoses 
##   Check linearity and homoscedasticity
plot(cs3.lm.a$fitted.values, cs3.lm.a$residuals, 
     pch=16, 
     main="residual plot")
abline(h=0, lwd= 4, col="red")  

##  Check normality 
qqnorm(cs3.lm.a$residuals)
qqline(cs3.lm.a$residuals, lwd=4, col="blue")

```

### answer: effect of time on MPG

From the simple regression of `mpg` on `year` we see that year is statistically significant at the .05 level. According to this model, one additional year would increase the mpg by 1.2, though could range from 1.06 to 1.4. This only exlplains \~40% of the variance. Something else to note is that the model's tails do begin to violate normality.

b)  Add `horsepower` on top of the variable `year` to your linear model. Is `year` still a significant variable at the .05 level? Give a precise interpretation of the `year`'s effect found here.

```{r add horsepower, message=FALSE, warning=FALSE}
# take one 
cs3.lm.b<-lm(mpg~year+horsepower, data=cs3.auto)
print(summary(cs3.lm.b))
confint(cs3.lm.b, level=.95)
#plot model diagnoses 
##   Check linearity and homoscedasticity
plot(cs3.lm.b$fitted.values, cs3.lm.b$residuals, 
     pch=16, 
     main="residual plot")
abline(h=0, lwd= 4, col="red")  

##  Check normality 
qqnorm(cs3.lm.b$residuals)
qqline(cs3.lm.b$residuals, lwd=4, col="blue")
# try it with the adjusted horsepower data
cs3.lm.badj<-lm(mpg~year+horse.2, data=cs3.auto)
print(summary(cs3.lm.badj))
confint(cs3.lm.b, level=.95)
#plot model diagnoses 
##   Check linearity and homoscedasticity
plot(cs3.lm.badj$fitted.values, cs3.lm.badj$residuals, 
     pch=16, 
     main="residual plot")
abline(h=0, lwd= 4, col="red")  

##  Check normality 
qqnorm(cs3.lm.badj$residuals)
qqline(cs3.lm.badj$residuals, lwd=4, col="blue")

```

### answer: adding horsepower to the model

Yes, year is still significant at the .05 level. In this model, we would observe that holding the `horsepower` of a car constant, every additional `year` increases the mpg by 0.66 mpg, with a 95% confidence interval ranging from .53, to .79. This version of the model explains \~69% of the variance and has a lower RSE, so it does seem to be a better model fit overall. THat being said, the model diagnoses show that there is a bit of a pattern to the residuals, suggesting that linearity and homoscedasticity could be under threat.

We took this as an opportunity to try our adjusted horsepower data, and when using the adjusted horsepower data, we actually see an even lower RSE at 3.51, and \~80% of variance explained. The normality and homoscedasticity plot looks even better, so this may be a stronger model to keep, or there may be other synergies.

c)  The two 95% CI's for the coefficient of year differ among (i) and (ii). How would you explain the difference to a non-statistician? \#\#\# answer: explain the different Confidence Intervals to a non-statistician Yes, the CI's are different for `year` in each model. I would tell a non-statistician that by adding another variable to the model we reduced the ambiguity in how mpg varies, and were more accurate in how much of that ambiguity is explained by `year`. This is why the range went from .34 to .26.

d)  Create a model with interaction by fitting `lm(mpg ~ year * horsepower)`. Is the interaction effect significant at .05 level? Explain the year effect (if any).

```{r interaction effect, message=FALSE, warning=FALSE}
cs3.lm.d<-lm(mpg~year*horsepower, data=cs3.auto)
print(summary(cs3.lm.d))

# model diagnoses 
confint(cs3.lm.b, level=.95)
#plot model diagnoses 
##   Check linearity and homoscedasticity
plot(cs3.lm.d$fitted.values, cs3.lm.b$residuals, 
     pch=16, 
     main="residual plot")
abline(h=0, lwd= 4, col="red")  

##  Check normality 
qqnorm(cs3.lm.d$residuals)
qqline(cs3.lm.d$residuals, lwd=4, col="blue")
```

### answer: interaction effect of horsepower on year

Based on this model, yes, the interaction effect is statistically significant at the .05 level. We see all variables have p-values nearing zero which allows us to reject the null hypothesis, and the F-statistic itself is quite large. We can interpret year as: an increase in the year is associated with a change in mpg equivalent to (2.19e+00 + -1.6e-02) x horsepower. The model does fit well, with 75% of variance explained. The model diagnoses appear slightly better than under the prior model.Practically, this effect is not that substantial.

## Categorical predictors

Remember that the same variable can play different roles! Take a quick look at the variable `cylinders`, and try to use this variable in the following analyses wisely. We all agree that a larger number of cylinders will lower mpg. However, we can interpret `cylinders` as either a continuous (numeric) variable or a categorical variable.

a)  Fit a model that treats `cylinders` as a continuous/numeric variable. Is `cylinders` significant at the 0.01 level? What effect does `cylinders` play in this model?

```{r cylinders part a, message=FALSE, warning=FALSE}
class(cs3.auto$cylinders)
cs3.lm.cat1<- lm(mpg~cylinders, data=cs3.auto)
cs3.lm.cat1.s<-summary(cs3.lm.cat1)
confint(cs3.lm.cat1)
print(cs3.lm.cat1.s)
#plot model diagnoses 
##   Check linearity and homoscedasticity
plot(cs3.lm.cat1$fitted.values, cs3.lm.cat1$residuals, 
     pch=16, 
     main="residual plot")
abline(h=0, lwd= 4, col="red")  

##  Check normality 
qqnorm(cs3.lm.cat1$residuals)
qqline(cs3.lm.cat1$residuals, lwd=4, col="blue")

```

### Answer: Numerical Cylinders

With Cylinders as a numerical variable, it is statistically significant at the .01 level, and for every additional cylinder, the mpg goes down by \~3.5 mpg, though it could be as much as 3.8, or as little as 3.27. The model displays decent fit with `r cs3.lm.cat1.s$r.squared` of the variance explained. The model diagnoses are good, but do suggest that this may be more of a categorical dataset.

b)  Fit a model that treats `cylinders` as a categorical/factor. Is `cylinders` significant at the .01 level? What is the effect of `cylinders` in this model? Describe the `cylinders` effect over `mpg`.

```{r categorical cylinders, warning=FALSE, message=FALSE}
#refactor the data
cs3.auto$fact.cyl<-as.factor(cs3.auto$cylinders)
#run the model 
cs3.lm.cat2<- lm(mpg~fact.cyl, data=cs3.auto)
#confirm the levels to see what the default is 
levels(cs3.auto$fact.cyl)
cs3.lm.cat2.s<-summary(cs3.lm.cat2)
print(cs3.lm.cat2.s)
confint(cs3.lm.cat2)
#check the assumptions 
#plot model diagnoses 
##   Check linearity and homoscedasticity
plot(cs3.lm.cat2$fitted.values, cs3.lm.cat2$residuals, 
     pch=16, 
     main="residual plot")
abline(h=0, lwd= 4, col="red")  

##  Check normality 
qqnorm(cs3.lm.cat2$residuals)
qqline(cs3.lm.cat2$residuals, lwd=4, col="blue")
# Check with Anova
pacman::p_load(car)
Anova(cs3.lm.cat2)
```

### Answer: Categorical Cylinders

From the new, categorical model, we see that the statistical significance varies depending on which cylinder count we are considering. In this model:

-   3-cylinder vehicle would, on average, have 20.5 mpg, ranging from 16 to 25 mpg.

-   Relative to a 3 cylinder vehicle

    -   a 4 cylinder vehicle would increase mpg by 8.7 (ranging from 4 to 13)

    -   a 5 cylinder vehicle would increase mpg less, by only 6.8 ( or possibly decreasing mpg by 0.24 up to an increase of 13.87)

    -   a 6 cylinder vehicle has no statistically significant effect -\> we cannot reject the null that there is no relationship, and

    -   an 8 cylinder vehicle is statically significant in that it will, on average, decrease the mpg by 5.6 (as much as 10 or as little as 1 mpg decrease.

The model as a whole is still statistically significant based on the F-score, and displays good fit with `r cs3.lm.cat2.s$r.squared`

c)  What are the fundamental differences between treating `cylinders` as a continuous and categorical variable in your models?

-   The fundamental differences between `cylinders` as continuous and as categorical are: As a continuous variable, we are imposing linearity, normality of residuals, and homoscedasticity, and looking for an average effect, assuming the same intercept. In other words, the effects we see are across all categories, and some cylinder vehicles may have very different mpg, all else equal, and we would not see this effect because it would be grouped into a larger coefficient. This is because the coefficient we get back is what we would expect on average for an increase in cylinders.

-   In contrast, with a a categorical variable is 'flexible' in that it says, we can hold all else equal, and look at the effect of cylinders between different groups. By allowing the intercept to shift, we are parsing out the specific effect of a given cylinder class.

d)  Can you test the null hypothesis: fit0: `mpg` is linear in `cylinders` vs. fit1: `mpg` relates to `cylinders` as a categorical variable at .01 level?

```{r test cat nulls, message=FALSE, warning=FALSE}
# Recall linear model
cs3.lm.cat1 
# Recall categorical model 
cs3.lm.cat2
# use little a anova to compare F-Tests 
cs3.anov.d<-anova(cs3.lm.cat1, cs3.lm.cat2) 
cs3.anov.d
```

### Answer: Compare Linear to Categorical

Using little A `anova()` we compared the linear model to the categorical model. This yielded a F-Statistic of `r cs3.anov.d$F`which was statistically significant with a p-value of `r cs3.anov.d$Pr(>F)`. At any reasonable level we reject the null hypothesis that the model should be linear, and we instead use the categorical model.

## Results

Final modeling question: we want to explore the effects of each feature as best as possible. You may explore interactions, feature transformations, higher order terms, or other strategies within reason. The model(s) should be as parsimonious (simple) as possible unless the gain in accuracy is significant from your point of view.

```{r final results, warning=FALSE, message=FALSE}
options(scipen=8)
#Recall names
names(cs3.auto)
#build data table for final model
cs3.auto.2<- cs3.auto %>% select(-disp.2, -horse.2, -cylinders, -name) %>% mutate(origin=factor(origin))
# fit all
fit.all<-lm(mpg~., cs3.auto.2)
fit.all.s<-summary(fit.all)
print(fit.all.s)
# remove acceleration - weird to interpret and not statistically significant
cs3.auto.3<- cs3.auto.2 %>% select (-acceleration)
fit.all.2<- lm(mpg~.,data=cs3.auto.3)
fit.all.2.s<-summary(fit.all.2)
print(fit.all.2.s)# model looks like everything is statistically significant. 
# Check Anova 
Anova(fit.all.2) # all good

# Model Diagnostics
##   Check linearity and homoscedasticity
plot(fit.all.2$fitted.values, fit.all.2$residuals, 
     pch=16, 
     main="residual plot")
abline(h=0, lwd= 4, col="red")  # problem, there is a fan. 

##  Check normality 
qqnorm(fit.all.2$residuals)
qqline(fit.all.2$residuals, lwd=4, col="blue")

#use logs for non-linear data 
cs3.auto.4<-cs3.auto.3 %>% 
   mutate(disp.2=log(displacement), 
          horse.2=log(horsepower)) %>% 
  select(-displacement, -horsepower)

#rerun model
fit.all.3<-lm(mpg~., data=cs3.auto.4)
fit.all.3.s<-summary(fit.all.3)
print(fit.all.3.s)

# Run ANOVA to check on origin overall 
Anova(fit.all.3) # looks like we still want it.

#remove log.displacement due to low value. 
cs3.auto.5<-cs3.auto.4 %>% select(-disp.2)
fit.all.4<-lm(mpg~., data=cs3.auto.5)
fit.all.4.s<-summary(fit.all.4)
print(fit.all.4.s)

Anova(fit.all.4) # looks good
# Model Diagnostics
##   Check linearity and homoscedasticity
plot(fit.all.4,1)
 # NOT BETTER

# use pairs to see what's up 
pairs(cs3.auto.5) # mpg is still having a non-linear relationship with other variables. will log it. 

fit.all.5<-lm(log(mpg)~., data=cs3.auto.5)
fit.all.5.s<-summary(fit.all.5)
print(fit.all.5.s)# looks good

#Anova
Anova(fit.all.5) #looks good
# Model Diagnostics
##   Check linearity and homoscedasticity
plot(fit.all.5$fitted.values, fit.all.5$residuals, 
     pch=16, 
     main="residual plot")
abline(h=0, lwd= 4, col="red")  # problem, there is a fan. 

##  Check normality 
qqnorm(fit.all.5$residuals)
qqline(fit.all.5$residuals, lwd=4, col="blue")# Like this a lot more. 

print(fit.all.5.s)
# one last test

fit.all.6<-lm(log(mpg)~weight + year+ fact.cyl + horse.2, data=cs3.auto.5) # remove origin for ease of interpretation 
fit.all.6.s<-summary(fit.all.6)
print(fit.all.6.s)# looks good Compared to model with origin, the variance explained is nearly the same. 

#Anova
Anova(fit.all.6) #looks good

# Model Diagnostics
##   Check linearity and homoscedasticity
plot(fit.all.6$fitted.values, fit.all.6$residuals, 
     pch=16, 
     main="residual plot")
abline(h=0, lwd= 4, col="red")  # problem, there is a fan. 

##  Check normality 
qqnorm(fit.all.6$residuals)
qqline(fit.all.6$residuals, lwd=4, col="blue")# Like this a lot more. 
# print final models for comparison 
print(fit.all.5.s)
print(fit.all.6.s)

## one last last test

fit.all.7<-lm(log(mpg)~ year+ fact.cyl + horse.2, data=cs3.auto.5) # remove weight because it did nothing 
fit.all.7.s<-summary(fit.all.7)
print(fit.all.7.s)# lost .05 of r.squared and cylinder is now in question

#Anova
Anova(fit.all.7) #cylinder looks good

# Model Diagnostics
##   Check linearity and homoscedasticity
plot(fit.all.7$fitted.values, fit.all.6$residuals, 
     pch=16, 
     main="residual plot")
abline(h=0, lwd= 4, col="red")  # problem, there is a fan. 

##  Check normality 
qqnorm(fit.all.7$residuals)
qqline(fit.all.7$residuals, lwd=4, col="blue")# Like this a lot 

print(fit.all.7.s)
intercept.all.7<- data.frame(fit.all.7.s$coefficients)
intercept.all.7<-intercept.all.7 %>% 
  select(Estimate) %>% 
  mutate(exp.coeff=exp(Estimate))
intercept.all.7
```

a)  Describe the final model. Include diagnostic plots with particular focus on the model residuals and diagnoses.

    ### Answer: final model

    In our final model (above), we predicted the log of mpg using year, categorical cylinders, and the log of horsepower. The model is a very close fit to the data, with an R.Squared of .85, and a relatively small RSE of 0.133. The overall model F-Statistic is quite high, suggesting that we can reject the null hypothesis that this model is not useful. A few points about our model selection:

-   A significant benefit of our model is that it has very high accuracy with only three variables.

-   Our model is slightly more difficult to interpret because we logged both the response and a predcitor variable. We did this because the original model displayed a fan towards the tail of the residual plot which remained present even after logging non-linear predictors. Using `pairs`, we saw that mpg was causing non-linearity and so had to log it as well. This means that to interpret the model we need to caveat that a one unit increase in year is associated with a 1% increase in mpg, for example.

-   In arriving at our final model, we first removed variables which were non-sensical, including `name` and the linear form of cylinders. We then worked backwards, and adjusted the variables to help fix the residual plots. This led us to a model which had higher predictive power because it included origin. However, upon reviewing the summary of that model, we saw that it would be complicated to explain a model with two sets of categorical variables, and retested the model without origin, given that origin had a smaller practical effect than cylinders. The change in R.squared was only .003, and the increase in RSE only .002, which suggests to us that the simpler model is worth the marginal loss of predictive power.

-   We then reviewed the model for practical significance and saw that weight had a minimal effect on MPG, and removed this as well. THe drop was a little higher, and r.squared went down \~.05 to .848. Overall this drop was not so substantial that we felt it was worth keeping weight in the model.

-   The final diagnostic plots show we meet the assumptions of normality, homoscedasticity, and linearity.

b)  Summarize the effects found.

-   each additional year raises the mpg by \~3%,

    -   exp(0.0259)=1.026

        -   (1.026-1) \* 100 = 3%

-   each 1% increase in horsepower decreases mpg by 4%

    -   because both are logged: 1.01\^-.4284 =.95 -1= \~4%

-   A 3-cylinder vehicle would have a mpg \~21. Relative to this,

    -   a 4-cylinder vehicle would be \~20% higher mpg

    -   a 5-cylinder vehicle would be \~9% higher mpg

    -   a 6-cylinder vehicle would be \~4% lower mpg

    -   an 8-cylinder vehicle would be \~9% lower mpg

c)  Predict the `mpg` of the following car: A red car built in the US in 1983 that is 180 inches long, has eight cylinders, displaces 350 cu. inches, weighs 4000 pounds, and has a horsepower of 260. Also give a 95% CI for your prediction.
```{r predict, warning=FALSE, message=FALSE}
#recall the model 
fit.all.7

# create dataset with the associated info
cs3.predict<-cs3.auto.5[1,]
cs3.predict <- cs3.predict %>% 
  select(-weight, -origin) %>% 
  mutate(
    year=83, 
    fact.cyl=as.factor(8), 
    horse.2=log(260)
  )
cs3.predict.lm<-predict(fit.all.7, cs3.predict, interval="confidence", se.fit=T)
prediction<-data.frame(cs3.predict.lm) %>% 
  mutate(prediction=exp(fit.fit),
         predict.lwr=exp(fit.lwr), 
         predict.upr=exp(fit.upr))
prediction %>% select(prediction, predict.lwr, predict.upr)

```


# Simple Regression through simulations

## Linear model through simulations

This exercise is designed to help you understand the linear model using simulations. In this exercise, we will generate $(x_i, y_i)$ pairs so that all linear model assumptions are met.

Presume that $\mathbf{x}$ and $\mathbf{y}$ are linearly related with a normal error $\boldsymbol{\varepsilon}$ , such that $\mathbf{y} = 1 + 1.2\mathbf{x} + \boldsymbol{\varepsilon}$. The standard deviation of the error $\varepsilon_i$ is $\sigma = 2$.

We can create a sample input vector ($n = 40$) for $\mathbf{x}$ with the following code:

```{r, eval = TRUE, echo = TRUE}
# Generates a vector of size 40 with equally spaced values between 0 and 1, inclusive
x <- seq(0, 1, length = 40)
```

### Generate data

Create a corresponding output vector for $\mathbf{y}$ according to the equation given above. Use `set.seed(1)`. Then, create a scatterplot with $(x_i, y_i)$ pairs. Base R plotting is acceptable, but if you can, please attempt to use `ggplot2` to create the plot. Make sure to have clear labels and sensible titles on your plots.

```{r}
pacman::p_load(gridExtra, ggrepel)
# TODO
set.seed(1)
e<-rnorm(40,sd=2)

# create corresponding vector 
cs4.data<-data.frame(x) 
cs4.data<-cs4.data %>% 
  mutate(y=1+1.2*x+e)
cs4.data
# Scatter plot 
cs4.p1<-cs4.data %>% 
  ggplot()+
  geom_point(aes(x=x, y=y ))+
  labs(title = "Random Plot", y="y=1+1.2(x)+e")
(cs4.p1)
```

### Understand the model

i.  Find the LS estimates of $\boldsymbol{\beta}_0$ and $\boldsymbol{\beta}_1$, using the `lm()` function. What are the true values of $\boldsymbol{\beta}_0$ and $\boldsymbol{\beta}_1$? Do the estimates look to be good?
```{r understand the model, warning=FALSE, message=FALSE}
cs4.lm1<-lm(y~x, data=cs4.data)
cs4.lm.s<-summary(cs4.lm1)
print(cs4.lm.s)
```
No, the estimates do not appear to be good. We have a statistically insignificant predictor, x, the model is insignificant, and the r.squared is 0.028. 

ii. What is your RSE for this linear model fit? Is it close to $\sigma = 2$?
It is 1.73. This is relatively close to sigma=2. 

iii. What is the 95% confidence interval for $\boldsymbol{\beta}_1$? Does this confidence interval capture the true $\boldsymbol{\beta}_1$?
```{r confint cs4, warning=FALSE, message=FALSE}
confint(cs4.lm1)
```
The confidence interval is -1 to 2.85. It does capture the true Beta One of 1.2. 


iv. Overlay the LS estimates and the true lines of the mean function onto a copy of the scatterplot you made above.

```{r}
cs4.data$ols.y<-cs4.lm1$fitted.values
cs4.data.long<- cs4.data %>% 
  mutate(
    actual.y=y) %>% 
         select(-y) %>% 
  mutate(
    obs=seq(1,40,length=40)) %>% 
  pivot_longer(
    cols= c(actual.y,ols.y), 
    names_to=c("source"), 
    values_to=c("number"))
coefs<- cs4.lm.s$coefficients[,1]# create coefs for plot
cs4.p2 <- cs4.data.long %>% 
  ggplot(aes(x=x, y=number, group=source, fill=source, label=obs))+
  geom_point(aes(color=source)) + 
  geom_text_repel() + 
  theme_classic()+ 
  labs(title="plot of actual vs. estimated y values", x="actual x values", y="actual and estimated y values")+ 
  geom_abline(intercept=coefs[1], slope=coefs[2], lwd=1, color="seagreen")+ 
  geom_abline(intercept=1, slope=(1.2*cs4.data$x+e), color="red")
#@maddie and Charles - this one plot below is the one that I'm lease confident in. I can't decide if the red lines should all be one line....
cs4.p2
```

### diagnoses

i.  Provide residual plot where fitted $\mathbf{y}$-values are on the x-axis and residuals are on the y-axis.

ii. Provide a normal QQ plot of the residuals.

iii. Comment on how well the model assumptions are met for the sample you used.

```{r}
# TODO
par(mfrow=c(1,2))
plot(cs4.lm1,1)
plot(cs4.lm1,2)
```
This particular model does not fit the data that well. Our assumption of linearity, and homoscedasticity are not met in the residuals plot -> you see a fan, and the line does not follow at h=0. The qqplot is okay, but it does waver a bit. 
## Understand sampling distribution and confidence intervals

This part aims to help you understand the notion of sampling statistics and confidence intervals. Let's concentrate on estimating the slope only.

Generate 100 samples of size $n = 40$, and estimate the slope coefficient from each sample. We include some sample code below, which should guide you in setting up the simulation. Note: this code is easier to follow but suboptimal; see the appendix for a more optimal R-like way to run this simulation.

```{r, eval = TRUE, echo = TRUE}
# Inializing variables. Note b_1, upper_ci, lower_ci are vectors
x <- seq(0, 1, length = 40) 
n_sim <- 100             # number of simulations
b1 <- 0                   # n_sim many LS estimates of beta_1 (=1.2). Initialize to 0 for now
upper_ci <- 0             # upper bound for beta_1. Initialize to 0 for now.
lower_ci <- 0             # lower bound for beta_1. Initialize to 0 for now.
t_star <- qt(0.975, 38)   # Food for thought: why 38 instead of 40? What is t_star?

# Perform the simulation
for (i in 1:n_sim){
  y <- 1 + 1.2 * x + rnorm(40, sd = 2)
  lse <- lm(y ~ x)
  lse_output <- summary(lse)$coefficients
  se <- lse_output[2, 2]
  b1[i] <- lse_output[2, 1]
  upper_ci[i] <- b1[i] + t_star * se
  lower_ci[i] <- b1[i] - t_star * se
}
results <- as.data.frame(cbind(se, b1, upper_ci, lower_ci))

# remove unecessary variables from our workspace
rm(se, b1, upper_ci, lower_ci, x, n_sim, b1, t_star, lse, lse_out) 
```

i.  Summarize the LS estimates of $\boldsymbol{\beta}_1$ (stored in `results$b1`). Does the sampling distribution agree with theory?
```{r sampling distribution, message=FALSE, warning=FALSE}
summary(results$b1)

```
The sampling distribution comes out as approximately normal. It does come close to the theory in that the mean and median are approximately normal. We know that beta1 is actually equal to 1.2, and this is not so far from 1.2 that we would be surprised, but it is also not exactly right. THis may be because 100 samples was not enough. Indeed, when we re-ran the program with n_sim=1,000, we did get a mean and medain of 1.22, which was much closer. So, from this we can see that a sample of only 40 is not bad, but it is still an approximation. Also, even increasing the number of simulations to 10,000, we do not get closer to the true value than we did at 1000. This shows the power of the models. Finally, when we rerun the simulation many times at n=100, we do see that occasionally the mean and median are accurate. 



ii. How many of your 95% confidence intervals capture the true $\boldsymbol{\beta}_1$? Display your confidence intervals graphically.

```{r graphical 95, message=FALSE, warning=FALSE}
results <- results %>% mutate(
  test=if_else(1.2>= lower_ci & 1.2<=upper_ci, "true", "false"), 
  count=seq(1,100, by=1))
results.longer<-results %>% pivot_longer(
  cols=c(upper_ci, lower_ci, b1), 
  names_to="source", 
  values_to="numbers")

cs4.falsecount<-table(results$test)[1]
print(cs4.falsecount)
cs4.p3<- results.longer %>% filter(source!="b1") %>% 
  ggplot(aes(x=count, group=count, y=numbers, label=count, shape = source))+
  geom_point(aes(color=source))+
  labs(title="Comparing Simulated Confidence Intervals with True Beta.One", x="unique simulation", y="")+ 
  geom_hline(yintercept=1.2, lwd=2, color="plum1")
cs4.p3

```
Only `r cs4.falsecount` confidence intervals missed the true beta, centered on the plum line above. Perhaps this is why in the original model we saw that the p-value was not statistically significant. 


